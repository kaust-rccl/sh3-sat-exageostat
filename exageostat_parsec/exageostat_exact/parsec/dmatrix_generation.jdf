extern "C" %{
/*
 * Copyright (c) 2017-2018 The Universiy of Tennessee and The Universiy
 *                         of Tennessee Research Foundation. All rights
 *                         reserved.
 */

#include "mix_precision_internal.h"

#if defined(EXAGEOSTAT_USE_CUDA)
#include "cudacore/include/exageostatcudacore.h"
#endif

/* Print more info */
static int print_more_gpu = 0;
static int print_more = 0;

static double deg2rad(double deg) {
        return (deg * PI / 180);
}       

static double distanceEarth(double lat1d, double lon1d, double lat2d, double lon2d) {
        double lat1r, lon1r, lat2r, lon2r, u, v;
        lat1r = deg2rad(lat1d);
        lon1r = deg2rad(lon1d);
        lat2r = deg2rad(lat2d);
        lon2r = deg2rad(lon2d);
        u = sin((lat2r - lat1r)/2);
        v = sin((lon2r - lon1r)/2);
        return 2.0 * earthRadiusKm * asin(sqrt(u * u + cos(lat1r) * cos(lat2r) * v * v));
}  

static double calculateDistance(double x1, double y1, double x2, double y2, int distance_metric) {
                                
        if(distance_metric == 1)
                return distanceEarth(x1, y1, x2, y2);
        return  sqrt(pow((x2 - x1), 2) + pow((y2 - y1), 2));
}

static void parsec_core_dcmg (double *A, int m, int n, int m0, int n0,
		location  *l1, location *l2, double *localtheta, int distance_metric, int lda) {

        int i, j;
        int i0 = m0;     
        int j0 = n0;     
        double x0, y0;
        double expr = 0.0;
        double con = 0.0;
        double sigma_square = localtheta[0];
        
        con = pow(2,(localtheta[2]-1)) * tgamma(localtheta[2]);
        con = 1.0/con;
        con = sigma_square * con;
        
        for (i = 0; i < m; i++) {
                j0 = n0;
                x0 = l1->x[i0];
                y0 = l1->y[i0];
                for (j = 0; j < n; j++) {
                        expr = calculateDistance(x0, y0, l2->x[j0], l2->y[j0], distance_metric)/localtheta[1];
                        if(expr == 0)
                                A[i + j * lda] = sigma_square + 1e-2;
                        else
                                A[i + j * lda] = con*pow(expr, localtheta[2])*gsl_sf_bessel_Knu(localtheta[2],expr); 
                        j0++;
                }
                i0++;  
        }
}


static void parsec_core_dcmg_pow_exp (double *A, int m, int n,
                int m0, int n0,
                location  *l1, location *l2, 
                double *localtheta, int distance_metric, int lda) {
    
        int i, j;
        int i0 = m0;
        int j0 = n0;
        double x0, y0;
        double expr  = 0.0;
        double expr1 = 0.0;
        double sigma_square = localtheta[0];
    
    
        for (i = 0; i < m; i++) {
                j0 = n0;
                x0 = l1->x[i0];
                y0 = l1->y[i0];
                for (j = 0; j < n; j++) {
                        expr  = calculateDistance(x0, y0, l2->x[j0], l2->y[j0], distance_metric);
                        expr1 = pow(expr, localtheta[2]);


                        if(expr == 0)
                                A[i + j * lda] = sigma_square /*+ 1e-4*/;
                        else
                                A[i + j * lda] = sigma_square *  exp(-(expr1/localtheta[1])) ; 
                        j0++;
                }
                i0++;
        }
}


%}

descA                [ type = "parsec_tiled_matrix_dc_t *" ]
l1                   [ type = "location *" ]
l2                   [ type = "location *" ]
theta                [ type = "double *" ]
dm                   [ type = "char *" ]
c_fun                [ type = "char *" ]
band_size_double     [ type = "int" ]

ws_l1_x           [ type = "void *" hidden = on default = NULL ]
ws_l1_y           [ type = "void *" hidden = on default = NULL ]
ws_l1_z           [ type = "void *" hidden = on default = NULL ]
ws_l2_x           [ type = "void *" hidden = on default = NULL ]
ws_l2_y           [ type = "void *" hidden = on default = NULL ]
ws_l2_z           [ type = "void *" hidden = on default = NULL ]

nb_cuda_devices      [ type = "int"   hidden = on default = 0 ]
cuda_device_index    [ type = "int *" hidden = on default = "NULL"]


/**************************************************
 *               bind_A                     *
 **************************************************/
bind_A(m, n)

// Execution space
n = 0 .. descA->lnt-1
m = %{ return ((descA->m == descA->n)? n: 0); %} .. descA->lmt-1

// Parallel partitioning
:descA(m, n)

READ A <- descA(m, n)
       -> A Task(m, n)

BODY
{
	int gpu_indicator = (strcmp(c_fun, "pow-exp") == 0)? 1 : 0;
#if defined(EXAGEOSTAT_USE_CUDA)
	if( nb_cuda_devices > 0 && gpu_indicator ) {
		int g = my_gpu_load( m, n, descA->nt, nb_cuda_devices, band_size_double, band_size_double); 
		parsec_advise_data_on_device( _f_A->original,
				cuda_device_index[g],
				PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE );
	}
#endif
}
END


Task(m, n)

n = 0 .. descA->lnt-1
m = %{ return ((descA->m == descA->n)? n: 0); %} .. descA->lmt-1 

gpu_indicator = %{ return ((strcmp(c_fun, "pow-exp") == 0)? 1 : 0); %}

: descA(m, n)

RW A <- A bind_A(m, n)
     -> descA(m, n)

BODY [type=CUDA weight=m+n+1]
{
#if defined(EXAGEOSTAT_USE_CUDA) 
	if( print_more_gpu ) printf("%d %d : GPU matrix generation\n", m, n);
        int tempmm = (m == descA->lmt-1) ? parsec_imin(descA->mb, descA->m-m*descA->mb): descA->mb;
        int tempnn = (n == descA->lnt-1) ? parsec_imin(descA->nb, descA->n-n*descA->nb): descA->nb;
        int m0 = m * descA->mb;
        int n0 = n * descA->nb;
        int distance_metric = (strcmp(dm, "gc") == 0)? 1: 0;  // Not used

        cudaError_t e;
        cublasSetKernelStream( parsec_body.stream );
	int size_l1 = descA->nb*sizeof(double);
	int size_l2 = descA->mb*sizeof(double);

        /* Get the temporary buffer on GPU */
        parsec_potrf_workspace_t *_ws_l1_x = (parsec_potrf_workspace_t *)ws_l1_x;
        parsec_potrf_stream_workspace_t stream_l1_x = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_l1_x);
        double *l1_x_cuda = (double *)stream_l1_x.gpu_buffer;
        assert(NULL != l1_x_cuda);

        parsec_potrf_workspace_t *_ws_l1_y = (parsec_potrf_workspace_t *)ws_l1_y;
        parsec_potrf_stream_workspace_t stream_l1_y = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_l1_y);
        double *l1_y_cuda = (double *)stream_l1_y.gpu_buffer;
        assert(NULL != l1_y_cuda);

        parsec_potrf_workspace_t *_ws_l2_x = (parsec_potrf_workspace_t *)ws_l2_x;
        parsec_potrf_stream_workspace_t stream_l2_x = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_l2_x);
        double *l2_x_cuda = (double *)stream_l2_x.gpu_buffer;
        assert(NULL != l2_x_cuda);

        parsec_potrf_workspace_t *_ws_l2_y = (parsec_potrf_workspace_t *)ws_l2_y;
        parsec_potrf_stream_workspace_t stream_l2_y = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_l2_y);
        double *l2_y_cuda = (double *)stream_l2_y.gpu_buffer;
        assert(NULL != l2_y_cuda);

        e = cudaMemcpy(l1_x_cuda, &l1->x[n0], size_l1, cudaMemcpyHostToDevice);
        e = cudaMemcpy(l1_y_cuda, &l1->y[n0], size_l1, cudaMemcpyHostToDevice);
        e = cudaMemcpy(l2_x_cuda, &l2->x[m0], size_l2, cudaMemcpyHostToDevice);
        e = cudaMemcpy(l2_y_cuda, &l2->y[m0], size_l2, cudaMemcpyHostToDevice);

#if OWN_GENERATION
	/* Only support 2D */
	assert( l1->z == NULL || l2->z == NULL ); 

        dcmg_array_GPU(A, tempmm, tempnn, m0, n0, l1_x_cuda, l1_y_cuda, l2_x_cuda,
                       l2_y_cuda, theta, distance_metric, descA->mb, parsec_body.stream);
#else
        if(l1->z == NULL || l2->z == NULL) {
		if( print_more_gpu ) fprintf(stderr, "2d\n");
		/* 2D */
		dcmg_array(A, descA->mb, descA->nb, m0, n0, l1_x_cuda, l1_y_cuda, l2_x_cuda,
				l2_y_cuda, theta, distance_metric, parsec_body.stream);
	} else {
		if( print_more_gpu ) fprintf(stderr, "3d\n");
		parsec_potrf_workspace_t *_ws_l1_z = (parsec_potrf_workspace_t *)ws_l1_z;
		parsec_potrf_stream_workspace_t stream_l1_z = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_l1_z);
		double *l1_z_cuda = (double *)stream_l1_z.gpu_buffer;
		assert(NULL != l1_z_cuda);

		parsec_potrf_workspace_t *_ws_l2_z = (parsec_potrf_workspace_t *)ws_l2_z;
		parsec_potrf_stream_workspace_t stream_l2_z = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_l2_z);
		double *l2_z_cuda = (double *)stream_l2_z.gpu_buffer;
		assert(NULL != l2_z_cuda);

		e = cudaMemcpy(l1_z_cuda, &l1->z[n0], size_l1, cudaMemcpyHostToDevice);
		e = cudaMemcpy(l2_z_cuda, &l2->z[m0], size_l2, cudaMemcpyHostToDevice);

		dcmg_array_3d(A, descA->mb, descA->nb, m0, n0, l1_x_cuda, l1_y_cuda, l1_z_cuda, l2_x_cuda,
				l2_y_cuda, l2_z_cuda, theta, distance_metric, parsec_body.stream);
	}
#endif

#endif
}
END

BODY
{
	if( print_more ) printf("%d %d : CPU matrix generation\n", m, n);
	int tempmm = (m == descA->lmt-1) ? parsec_imin(descA->mb, descA->m-m*descA->mb): descA->mb;
	int tempnn = (n == descA->lnt-1) ? parsec_imin(descA->nb, descA->n-n*descA->nb): descA->nb;
	int m0 = m * descA->mb;
	int n0 = n * descA->nb;
	int distance_metric = (strcmp(dm, "gc") == 0)? 1: 0;
	/* int covariance_fun = (strcmp(c_fun, "pow-exp") == 0)? 1 : 0; */
	int covariance_fun=0;
	if (strcmp(c_fun, "pow-exp-nuggets") == 0)
	    covariance_fun=2;
	else if (strcmp(c_fun, "pow-exp") == 0)
	    covariance_fun=1;
	else
	    covariance_fun=0;

#if OWN_GENERATION
	/* Only support 2D */
	assert( l1->z == NULL || l2->z == NULL ); 
        if(covariance_fun == 0) {
                parsec_core_dcmg(A, tempmm, tempnn, m0, n0, l1, l2, theta, distance_metric, descA->mb);
        } else if(covariance_fun == 1) {
                parsec_core_dcmg_pow_exp(A, tempmm, tempnn, m0, n0, l1, l2, theta, distance_metric, descA->mb);
        }
#else
        if(covariance_fun == 0) {
                core_dcmg(A, descA->mb, descA->nb, m0, n0, l1, l2, theta, distance_metric);
        } else if(covariance_fun == 1) {
                core_dcmg_pow_exp(A, descA->mb, descA->nb, m0, n0, l1, l2, theta, distance_metric);
        } if(covariance_fun == 2) {
	        core_dcmg_pow_exp_nuggets(A, descA->mb, descA->nb, m0, n0, l1, l2, theta, distance_metric);
	}

#endif

}
END

extern "C" %{

#if defined(EXAGEOSTAT_USE_CUDA) 
/* Select GPU Task kernel 
 * Can not pass internal_taskpool, so local instead
 */
static float evaluate_gpu_Task(parsec_task_t* task) {
    int gpu_indicator = ((__parsec_dmatrix_generation_Task_task_t *)task)->locals.gpu_indicator.value;
    if( gpu_indicator )
        return PARSEC_HOOK_RETURN_DONE;
    else
        return PARSEC_HOOK_RETURN_NEXT;
}

#if GPU_BUFFER_ONCE
extern parsec_potrf_workspace_t *ws_l1_x;
extern parsec_potrf_workspace_t *ws_l1_y;
extern parsec_potrf_workspace_t *ws_l1_z;
extern parsec_potrf_workspace_t *ws_l2_x;
extern parsec_potrf_workspace_t *ws_l2_y;
extern parsec_potrf_workspace_t *ws_l2_z;
#endif

#endif


/**
 * @param [in] dcA:    the data, already distributed and allocated
 * @return the parsec object to schedule.
 */
parsec_taskpool_t*
parsec_dmatrix_generation_New(parsec_context_t *parsec,
		parsec_tiled_matrix_dc_t *dcA, location *l1, location *l2, 
		double *theta , char *dm, char *c_fun, int band_size_double) 
{
    parsec_taskpool_t* dmatrix_generation_taskpool;
    parsec_dmatrix_generation_taskpool_t* taskpool = NULL;

#if defined(EXAGEOSTAT_USE_CUDA)
    /** Find all CUDA devices */
    int nb = 0;
    for(int i = 0; i < (int)parsec_nb_devices; i++) {
        parsec_device_module_t *device = parsec_mca_device_get(i);
        if( PARSEC_DEV_CUDA == device->type ) {
            nb++;
        }
    }

    if(nb == 0) {
        char hostname[256];
        gethostname(hostname, 256);
        if( print_more_gpu && 0 == dcA->super.myrank ) {
            fprintf(stderr, "\nWarnning: No CUDA device found on rank %d on %s\n\n",
                    parsec->my_rank, hostname);
        }
    } else {
        if( print_more_gpu && strcmp(c_fun, "pow-exp") && 0 == dcA->super.myrank )
            fprintf(stderr, "\nWarnning: Matrix generation will be done on CPU\n\n");
    }

    int *dev_index = (int*)malloc(nb * sizeof(int));
    nb = 0;
    for(int i = 0; i < (int)parsec_nb_devices; i++) {
        parsec_device_module_t *device = parsec_mca_device_get(i);
        if( PARSEC_DEV_CUDA == device->type ) {
            dev_index[nb++] = device->device_index;
        }
    }

#if !GPU_BUFFER_ONCE
    /* Declare workspace used on GPU */
    parsec_potrf_workspace_t *ws_l1_x, *ws_l1_y, *ws_l1_z, *ws_l2_x, *ws_l2_y, *ws_l2_z;

    /* Allocate memory */
    ws_l1_x = workspace_memory_allocate( ws_l1_x );
    ws_l1_y = workspace_memory_allocate( ws_l1_y );
    ws_l1_z = workspace_memory_allocate( ws_l1_z );
    ws_l2_x = workspace_memory_allocate( ws_l2_x );
    ws_l2_y = workspace_memory_allocate( ws_l2_y );
    ws_l2_z = workspace_memory_allocate( ws_l2_z );

    /* Traverse all gpu device */
    for(int i = 0; i < (int)parsec_nb_devices; i++) {
        parsec_device_module_t *device = parsec_mca_device_get(i);
        if( NULL == device ) continue;
        if( device->type != PARSEC_DEV_CUDA ) continue;

        parsec_device_cuda_module_t *gpu_device = (parsec_device_cuda_module_t*)device;
        cudaSetDevice(gpu_device->cuda_index);

        ws_l1_x->gpu_workspace[i].gpu_device = gpu_device;
        ws_l1_y->gpu_workspace[i].gpu_device = gpu_device;
        ws_l1_z->gpu_workspace[i].gpu_device = gpu_device;
        ws_l2_x->gpu_workspace[i].gpu_device = gpu_device;
        ws_l2_y->gpu_workspace[i].gpu_device = gpu_device;
        ws_l2_z->gpu_workspace[i].gpu_device = gpu_device;

        /* Traverse all streams */
        for(int j = 0; j < gpu_device->max_exec_streams; j++) {
            /* j 0, h2d; j 1, d2h */
            if( j <= 1 ) continue;

            /* Set unused to NULL */
            {
                ws_l1_x->gpu_workspace[i].stream_workspace[j].handle_cusolver = NULL;
                ws_l1_x->gpu_workspace[i].stream_workspace[j].handle_cublas = NULL;

                ws_l1_y->gpu_workspace[i].stream_workspace[j].handle_cusolver = NULL;
                ws_l1_y->gpu_workspace[i].stream_workspace[j].handle_cublas = NULL;

                ws_l1_z->gpu_workspace[i].stream_workspace[j].handle_cusolver = NULL;
                ws_l1_z->gpu_workspace[i].stream_workspace[j].handle_cublas = NULL;

                ws_l2_x->gpu_workspace[i].stream_workspace[j].handle_cusolver = NULL;
                ws_l2_x->gpu_workspace[i].stream_workspace[j].handle_cublas = NULL;

                ws_l2_y->gpu_workspace[i].stream_workspace[j].handle_cusolver = NULL;
                ws_l2_y->gpu_workspace[i].stream_workspace[j].handle_cublas = NULL;

                ws_l2_z->gpu_workspace[i].stream_workspace[j].handle_cusolver = NULL;
                ws_l2_z->gpu_workspace[i].stream_workspace[j].handle_cublas = NULL;
            }

            /* Temporary buffer for l1 */
            {
                int workspace_size = dcA->nb;
                ws_l1_x->gpu_workspace[i].stream_workspace[j].gpu_buffer = zone_malloc( gpu_device->memory, workspace_size * sizeof(double) );
                assert(NULL != ws_l1_x->gpu_workspace[i].stream_workspace[j].gpu_buffer);
                ws_l1_x->gpu_workspace[i].stream_workspace[j].buffer_size = workspace_size;

                ws_l1_y->gpu_workspace[i].stream_workspace[j].gpu_buffer = zone_malloc( gpu_device->memory, workspace_size * sizeof(double) );
                assert(NULL != ws_l1_y->gpu_workspace[i].stream_workspace[j].gpu_buffer);
                ws_l1_y->gpu_workspace[i].stream_workspace[j].buffer_size = workspace_size;

                ws_l1_z->gpu_workspace[i].stream_workspace[j].gpu_buffer = zone_malloc( gpu_device->memory, workspace_size * sizeof(double) );
                assert(NULL != ws_l1_z->gpu_workspace[i].stream_workspace[j].gpu_buffer);
                ws_l1_z->gpu_workspace[i].stream_workspace[j].buffer_size = workspace_size;
            }

            /* Temporary buffer for l2 */
            {
                int workspace_size = dcA->mb;
                ws_l2_x->gpu_workspace[i].stream_workspace[j].gpu_buffer = zone_malloc( gpu_device->memory, workspace_size * sizeof(double) );
                assert(NULL != ws_l2_x->gpu_workspace[i].stream_workspace[j].gpu_buffer);
                ws_l2_x->gpu_workspace[i].stream_workspace[j].buffer_size = workspace_size;

                ws_l2_y->gpu_workspace[i].stream_workspace[j].gpu_buffer = zone_malloc( gpu_device->memory, workspace_size * sizeof(double) );
                assert(NULL != ws_l2_y->gpu_workspace[i].stream_workspace[j].gpu_buffer);
		ws_l2_y->gpu_workspace[i].stream_workspace[j].buffer_size = workspace_size;

                ws_l2_z->gpu_workspace[i].stream_workspace[j].gpu_buffer = zone_malloc( gpu_device->memory, workspace_size * sizeof(double) );
                assert(NULL != ws_l2_z->gpu_workspace[i].stream_workspace[j].gpu_buffer);
		ws_l2_z->gpu_workspace[i].stream_workspace[j].buffer_size = workspace_size;
	    }

	}
    }
#endif /* #if GPU_BUFFER_ONCE */

#endif
    taskpool = parsec_dmatrix_generation_new(dcA, l1, l2, theta, dm, c_fun, band_size_double);
    dmatrix_generation_taskpool = (parsec_taskpool_t*)taskpool;

#if defined(EXAGEOSTAT_USE_CUDA)
    taskpool->_g_ws_l1_x = (void *)ws_l1_x;
    taskpool->_g_ws_l1_y = (void *)ws_l1_y;
    taskpool->_g_ws_l1_z = (void *)ws_l1_z;
    taskpool->_g_ws_l2_x = (void *)ws_l2_x;
    taskpool->_g_ws_l2_y = (void *)ws_l2_y;
    taskpool->_g_ws_l2_z = (void *)ws_l2_z;
    taskpool->_g_nb_cuda_devices = nb;
    taskpool->_g_cuda_device_index = dev_index;

    /* Select Kernel that run on GPU */
    void** eval_gpu;
    eval_gpu = (void *)&taskpool->super.task_classes_array[1]->incarnations[0].evaluate;
    *eval_gpu = &evaluate_gpu_Task;
#endif

    parsec_matrix_add2arena(taskpool->arenas[PARSEC_dmatrix_generation_DEFAULT_ARENA],
                            parsec_datatype_double_t, matrix_UpperLower,
                            1, dcA->mb, dcA->nb, dcA->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    return dmatrix_generation_taskpool;
}

/**
 * @param [inout] the parsec object to destroy
*/
void parsec_dmatrix_generation_Destruct(parsec_taskpool_t *taskpool)
{
    parsec_dmatrix_generation_taskpool_t *tp = (parsec_dmatrix_generation_taskpool_t *)taskpool;

#if defined(EXAGEOSTAT_USE_CUDA)
    if( tp->_g_nb_cuda_devices > 0 ) {
#if !GPU_BUFFER_ONCE
        workspace_memory_free( tp->_g_ws_l1_x );
        workspace_memory_free( tp->_g_ws_l1_y );
        workspace_memory_free( tp->_g_ws_l1_z );
        workspace_memory_free( tp->_g_ws_l2_x );
        workspace_memory_free( tp->_g_ws_l2_y );
        workspace_memory_free( tp->_g_ws_l2_z );
#endif

        if( NULL != tp->_g_cuda_device_index )
            free(tp->_g_cuda_device_index);
    }
#endif

    parsec_matrix_del2arena(tp->arenas[PARSEC_dmatrix_generation_DEFAULT_ARENA]);
    parsec_taskpool_free(taskpool);
}

/**
 * @brief allocate and generate dcA
 * 
 * @param [inout] dcA: the data, already distributed and allocated
 */
int parsec_dmatrix_generation(parsec_context_t *parsec,
                         parsec_tiled_matrix_dc_t *dcA, location *l1, location *l2,
                         double *theta , char *dm, char *c_fun, int band_size_double)
{
    parsec_taskpool_t *parsec_dmatrix_generation = NULL;

    parsec_dmatrix_generation = parsec_dmatrix_generation_New( 
                               parsec, dcA, l1, l2, theta, dm, c_fun, band_size_double); 

    if( parsec_dmatrix_generation != NULL ){
        parsec_context_add_taskpool(parsec, parsec_dmatrix_generation);
        parsec_context_start(parsec);
        parsec_context_wait(parsec);
        parsec_dmatrix_generation_Destruct(parsec_dmatrix_generation);
    }

    return 0;
}

%}
