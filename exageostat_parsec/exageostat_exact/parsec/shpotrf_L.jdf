extern "C" %{
/*
 * Copyright (c) 2010-2020 The University of Tennessee and The University
 *                         of Tennessee Research Foundation. All rights
 *                         reserved.
 */
#include "mix_precision_internal.h"
#include "include/dplasmajdf.h"

/* Print more info */
static int print_more = 0;
static int print_more_gpu = 0;

/*
 * Priorities used in this jdf:
 *      - shpotrf_shpotrf(k)    : (MT-k)**3
 *      - shpotrf_ssyrk(k,m)   : (MT-m)**3 + 3 * (m - k)
 *      - shpotrf_strsm(m,k)   : (MT-m)**3 + 3 * (m - k) * (2 * MT - k - m - 1)
 *      - shpotrf_sgemm(m,n,k) : (MT-m)**3 + 3 * (m - n) * (2 * MT - m - n - 1) + 6 * (m - k)
 *
 * So max priority is:
 *      (MT - PRI_CHANGE)**3 + 3 * MT * (2 * MT - PRI_CHANGE - 1) + 6 * MT  < (MT**3 + 6 MT**2 + 3 MT)
 *
 * WARNING: If mt is greater than 1200, we might get integer overflow.
 */

%}

/* Globals
 */
uplo       [type = PLASMA_enum]
descA      [type = "parsec_tiled_matrix_dc_t*"]
band_size  [type = "int"]
tensor_gemm  [type = "int"]
lookahead  [type = "int"]
INFO       [type = "int*"]

PRI_CHANGE [type = "int" hidden = on default = 0 ]
PRI_MAX    [type = "int" hidden = on default = "(descA->mt * ( 3 + descA->mt * ( 2 + descA->mt )))" ]
smallnb    [type = "int" hidden = on default = "descA->mb" ]

/* GPU workspace */
ws_handle_cusolver   [ type = "void *" hidden = on default = NULL ]
ws_handle_cublas_tensor     [ type = "void *" hidden = on default = NULL ]
ws_handle_cublas_gpu        [ type = "void *" hidden = on default = NULL ]
ws_single1           [ type = "void *" hidden = on default = NULL ]
ws_half1             [ type = "void *" hidden = on default = NULL ]
ws_half2             [ type = "void *" hidden = on default = NULL ]
ws_half3             [ type = "void *" hidden = on default = NULL ]

nb_cuda_devices      [ type = "int"   hidden = on default = 1 ]
cuda_device_index    [ type = "int *" hidden = on default = "NULL"]


/**************************************************
 *               shpotrf_bind_A                     *
 **************************************************/
shpotrf_bind_A(m, n)

// Execution space
m = 0 .. descA->nt-1
n = 0 .. m 

// Parallel partitioning
:descA(m, n)

READ A <- descA(m, n)
       -> (m == 0 && n == 0) ? T shpotrf_shpotrf(0)                   [ type = SINGLE ]
       -> (m > 0  && n == 0) ? C shpotrf_strsm(m, 0)   [ type = SINGLE ]
       -> (m == n && n > 0) ? T shpotrf_ssyrk(0, m)    [ type = SINGLE ]
       -> (m != n && n > 0) ? C shpotrf_sgemm(m, n, 0) [ type = SINGLE ]

BODY
{
#if defined(EXAGEOSTAT_USE_CUDA)
    if( nb_cuda_devices > 0 ) {
        int g = my_gpu_load( m, n, descA->nt, nb_cuda_devices, band_size, band_size);
        parsec_advise_data_on_device( _f_A->original,
                                    cuda_device_index[g],
                                    PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE );
    }
#endif
}
END


/**************************************************
 *               shpotrf_shpotrf                     *
 **************************************************/
shpotrf_shpotrf(k) [high_priority = on] 

// Execution space
k = 0 .. descA->mt-1

// Parallel partitioning
:descA(k, k)

// Parameters
RW T <- (k == 0) ? A shpotrf_bind_A(k, k) : T shpotrf_ssyrk(k-1, k)
     -> T shpotrf_strsm(k+1..descA->mt-1, k)
     -> descA(k, k)

; (k >= (descA->mt - PRI_CHANGE)) ? (descA->mt - k) * (descA->mt - k) * (descA->mt - k) : PRI_MAX

BODY [type=CUDA weight=k+1]
{
#if defined(EXAGEOSTAT_USE_CUDA)
    if( print_more_gpu ) printf("GPU_potrf %d ; nb_cuda_devices: %d ; cuda_device_index %d\n", k, nb_cuda_devices, gpu_device->cuda_index);

    int tempkn = k == descA->nt-1 ? descA->n - k*descA->nb : descA->nb;
    int ldak = descA->mb; 
    cusolverStatus_t status;

    /* Lookup workspace */
    parsec_potrf_workspace_t *_ws_handle = (parsec_potrf_workspace_t *)ws_handle_cusolver;
    parsec_potrf_stream_workspace_t stream_handle = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_handle);

    /* Info used on GPU kernel */
    cusolverDnHandle_t handle = stream_handle.handle_cusolver;
    int *dev_info = (int *)(stream_handle.gpu_buffer + stream_handle.buffer_size * sizeof(float));
    float *gpu_buffer = (float *)stream_handle.gpu_buffer;
    int buffer_size = stream_handle.buffer_size;
    assert(NULL != gpu_buffer);

    /* GPU kernel */
    status = cusolverDnSpotrf(handle, CUBLAS_FILL_MODE_LOWER, tempkn, T, ldak, gpu_buffer, buffer_size, dev_info);
    assert(CUSOLVER_STATUS_SUCCESS == status);

    /* We are losing the iinfo in d_iinfo, because the kernel is asynchronous.
     * We should register a complete function to read d_iinfo back in CPU memory,
     * and update INFO with it... */
#endif
}
END

BODY
{
    int tempkm = k == descA->mt-1 ? descA->m - k*descA->mb : descA->mb;
    int iinfo = 0;
    int ldak = descA->mb; 

#if !defined(PARSEC_DRY_RUN)
    CORE_spotrf( uplo, tempkm, T, ldak, &iinfo );
    if ( iinfo != 0 && *INFO == 0 )
            *INFO = k*descA->mb+iinfo; /* Should return here */
#endif /* !defined(PARSEC_DRY_RUN) */

    printlog("CORE_spotrf( %d )\n\t( %s, %d, A(%d,%d)[%p], %d) return info = %d\n",
             k,
             plasma_const(uplo), tempkm, k, k, T, descA->mb, iinfo );
}
END

/**************************************************
 *               shpotrf_strsm                      *
 **************************************************/
shpotrf_strsm(m, k) [high_priority = on] 

// Execution space
m = 1 .. descA->mt-1
k = 0 .. m-1

// Parallel partitioning
: descA(m, k)

// Parameters
READ  T <- T shpotrf_shpotrf(k)

RW    C <- (k == 0) ? A shpotrf_bind_A(m, k) : C shpotrf_sgemm(m, k, k-1)
        -> A shpotrf_ssyrk(k, m)
        -> A shpotrf_sgemm(m, k+1..m-1, k)
        -> B shpotrf_sgemm(m+1..descA->mt-1, m, k)
        -> descA(m, k)

CTL ctl1 <- (lookahead > 2 && m > lookahead+k)? ctl1 shpotrf_sgemm(k+2, k+1, k)

; (m >= (descA->mt - PRI_CHANGE)) ? (descA->mt - m) * (descA->mt - m) * (descA->mt - m) + 3 * ((2 * descA->mt) - k - m - 1) * (m - k) : PRI_MAX

BODY [type=CUDA weight=(m+1-k)]
{
#if defined(EXAGEOSTAT_USE_CUDA)
    int tempmm = m == descA->mt-1 ? descA->m - m * descA->mb : descA->mb;
    int ldak = descA->mb; 
    int ldam = descA->mb; 
    const float alpha = (float)1.0;

    /* Get handle_cublas */ 
    parsec_potrf_workspace_t *_ws_handle = (parsec_potrf_workspace_t *)ws_handle_cublas_gpu;
    parsec_potrf_stream_workspace_t stream_handle = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_handle);
    cublasHandle_t handle = stream_handle.handle_cublas;

    cublasStatus_t status;
    cublasSetStream( handle, parsec_body.stream );

    status = cublasStrsm( handle, CUBLAS_SIDE_RIGHT, CUBLAS_FILL_MODE_LOWER,
                          CUBLAS_OP_T, CUBLAS_OP_N,
                          tempmm, descA->mb,
                          &alpha, T /*A(k, k)*/, ldak,
                                  C /*A(m, k)*/, ldam);
#endif
}
END

BODY
{
    int tempmm = m == descA->mt-1 ? descA->m - m * descA->mb : descA->mb;
    int ldak = descA->mb; 
    int ldam = descA->mb; 

    CORE_strsm(PlasmaRight, PlasmaLower, PlasmaTrans, PlasmaNonUnit,
               tempmm, descA->nb,
               (float)1.0, T /*A(k, k)*/, ldak,
                                C /*A(m, k)*/, ldam);

    printlog("CORE_strsm( %d, %d )\n\t( %s, %s, %s, %s, %d, %d, %f, A(%d,%d)[%p], %d,  A(%d,%d)[%p], %d)\n",
             m, k,
             plasma_const( PlasmaRight ), plasma_const( PlasmaLower ),
             plasma_const( PlasmaTrans ), plasma_const( PlasmaNonUnit ),
             tempmm, descA->nb,
             1.0, k, k, T, ldak,
                  m, k, C, ldam);
}
END


/**************************************************
 *               shpotrf_ssyrk                      *
 **************************************************/
shpotrf_ssyrk(k, m) [high_priority = on]

// Execution space
k = 0   .. descA->mt-2
m = k+1 .. descA->mt-1

// Parallel partitioning
: descA(m, m)

//Parameters
READ  A <- C shpotrf_strsm(m, k)

RW    T <- (k == 0)   ? A shpotrf_bind_A(m, m) : T shpotrf_ssyrk(k-1, m)
        -> (m == k+1) ? T shpotrf_shpotrf(m)  : T shpotrf_ssyrk(k+1, m)

; (m >= (descA->mt - PRI_CHANGE)) ? (descA->mt - m) * (descA->mt - m) * (descA->mt - m) + 3 * (m - k) : PRI_MAX

BODY [type=CUDA weight=(m+1-k)]
{
#if defined(EXAGEOSTAT_USE_CUDA)
    int tempmm = m == descA->mt-1 ? descA->m - m*descA->mb : descA->mb;
    int ldam = descA->mb;
    const float alpha = (float)-1.0;
    const float beta = (float)1.0;
    if( print_more_gpu ) printf("GPU_syrk: %d %d : cuda_index %d\n", m, k, gpu_device->cuda_index);

    /* Get handle_cublas */
    parsec_potrf_workspace_t *_ws_handle = (parsec_potrf_workspace_t *)ws_handle_cublas_gpu;
    parsec_potrf_stream_workspace_t stream_handle = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_handle);
    cublasHandle_t handle = stream_handle.handle_cublas;

    cublasStatus_t status;
    cublasSetStream( handle, parsec_body.stream );

    status = cublasSsyrk( handle, CUBLAS_FILL_MODE_LOWER, CUBLAS_OP_N,
                    tempmm, descA->mb,
                    &alpha, A  /*A(m, k)*/, ldam,
                    &beta,  T  /*A(m, m)*/, ldam);
#endif
}
END

BODY
{
    int tempmm = m == descA->mt-1 ? descA->m - m*descA->mb : descA->mb;
    int ldam = descA->mb; 

#if !defined(PARSEC_DRY_RUN)
    CORE_ssyrk(PlasmaLower, PlasmaNoTrans,
               tempmm, descA->mb,
               (float)-1.0, A /*A(m, k)*/, ldam,
               (float) 1.0, T /*A(m, m)*/, ldam);
#endif  /* !defined(PARSEC_DRY_RUN) */

    printlog(
             "CORE_ssyrk( %d, %d )\n\t( %s, %s, %d, %d, %f, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             k, m,
             plasma_const( PlasmaLower ), plasma_const( PlasmaNoTrans ),
             tempmm, descA->mb,
             -1.0, m, k, A, ldam,
              1.0, m, m, T, ldam);
}
END

/**************************************************
 *               shpotrf_sgemm                      *
 **************************************************/
// Name
shpotrf_sgemm(m, n, k)

// Execution space
k = 0   .. descA->mt-3
m = k+2 .. descA->mt-1
n = k+1 .. m-1

// Parallel partitioning
: descA(m, n)

// Parameters
READ  A <- C shpotrf_strsm(m, k)
READ  B <- C shpotrf_strsm(n, k)

RW    C <- (k == 0) ? A shpotrf_bind_A(m, n) : C shpotrf_sgemm(m, n, k-1)
        -> (n == k+1) ? C shpotrf_strsm(m, n) : C shpotrf_sgemm(m, n, k+1)

CTL ctl1 -> (lookahead > 2 && m == k+2 && n == k+1)? ctl1 shpotrf_strsm(k+lookahead+1 .. descA->mt-1, k)

; (m >= (descA->mt - PRI_CHANGE)) ? (descA->mt - m) * (descA->mt - m) * (descA->mt - m) + 3 * ((2 * descA->mt) - m - n - 3) * (m - n) + 6 * (m - k) : PRI_MAX

BODY [type=CUDA weight=(n+1-k)]
{
#if defined(EXAGEOSTAT_USE_CUDA)
    int tempmm = m == descA->mt-1 ? descA->m - m * descA->mb : descA->mb;
    int ldam = descA->mb; 
    int ldan = descA->mb; 
    const float alpha = (float)-1.0;
    const float beta = (float)1.0;

    cublasStatus_t status;
    cudaError_t err;

    /* Get handle_cublas */
    parsec_potrf_workspace_t * _ws_handle = (parsec_potrf_workspace_t *)ws_handle_cublas_gpu;
    parsec_potrf_stream_workspace_t stream_handle = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_handle);
    cublasHandle_t handle = stream_handle.handle_cublas;

    cublasSetStream( handle, parsec_body.stream );

if( m-n < band_size ) {
    status = cublasSgemm( handle, CUBLAS_OP_N, CUBLAS_OP_T,
             tempmm, descA->mb, descA->mb,
             &alpha, A, ldam,
                     B, ldan,
             &beta,  C, ldam );
} else {
        float *B_s, *C_s;
        void *A_h, *B_h, *C_h;

        if( print_more_gpu ) printf("tensor_gemm %d %d %d : band_size %d, tensor_gemm %d \n", m, n, k, band_size, tensor_gemm);

        /* Get handle_cublas that to run on Tensor cores */
        parsec_potrf_workspace_t * _ws_handle = (parsec_potrf_workspace_t *)ws_handle_cublas_tensor;
        parsec_potrf_stream_workspace_t stream_handle = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_handle);
        cublasHandle_t handle = stream_handle.handle_cublas;

        cublasSetStream( handle, parsec_body.stream );

        /* Convert A from double to half */
        /* Get the temporary buffer on GPU */
        parsec_potrf_workspace_t *_ws_half1 = (parsec_potrf_workspace_t *)ws_half1;
        parsec_potrf_stream_workspace_t stream_half1 = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_half1);
        A_h = (void *)stream_half1.gpu_buffer;
        assert(NULL != A_h);

        /* Convert datatype */
        float2half_GPU( descA->mb, descA->nb, A, descA->mb, A_h, descA->mb, parsec_body.stream );

        /* Convert B from double to half */
        /* Get the temporary buffer on GPU */
        parsec_potrf_workspace_t *_ws_half2 = (parsec_potrf_workspace_t *)ws_half2;
        parsec_potrf_stream_workspace_t stream_half2 = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_half2);
        B_h = (void *)stream_half2.gpu_buffer;
        assert(NULL != B_h);

        /* Convert datatype */
        float2half_GPU( descA->mb, descA->nb, B, descA->mb, B_h, descA->mb, parsec_body.stream );

        /* First local GEMM convert C from single to half */
        if( 0 == k && ( 2 == tensor_gemm || 3 == tensor_gemm ) ) {
            /* Get the temporary buffer on GPU */
            parsec_potrf_workspace_t *_ws_half3 = (parsec_potrf_workspace_t *)ws_half3;
            parsec_potrf_stream_workspace_t stream_half3 = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_half3);
            C_h = (void *)stream_half3.gpu_buffer;
            assert(NULL != C_h);

            /* Convert datatype */
            float2half_GPU( descA->mb, descA->nb, C, descA->mb, C_h, descA->mb, parsec_body.stream );

            /* Copy C_h to C */
            memcpy_half_GPU( descA->mb, descA->nb, C_h, C, parsec_body.stream );
        }

        // TODO is CUBLAS_GEMM_DEFAULT_TENSOR_OP correct?
        switch( tensor_gemm ) {
            case 1:
                 /* AB16F_C32F_OP32F */
                 status = cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_T,
                                   (int64_t)tempmm, (int64_t)descA->mb, (int64_t)descA->mb,
                                   &alpha, A_h, CUDA_R_16F, (int64_t)ldam,
                                           B_h, CUDA_R_16F, (int64_t)ldan,
                                   &beta,  C,   CUDA_R_32F, (int64_t)ldam,
                                   CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP);
                 break;

            case 2:
                 /* AB16F_C16F_OP32F */
                 status = cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_T,
                                   (int64_t)tempmm, (int64_t)descA->mb, (int64_t)descA->mb,
                                   &alpha, A_h, CUDA_R_16F, (int64_t)ldam,
                                           B_h, CUDA_R_16F, (int64_t)ldan,
                                   &beta,  C,   CUDA_R_16F, (int64_t)ldam,
                                   CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP);
                 break;

            case 3:
                 /* AB16F_C16F_OP16F */
                 status = my_cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_T,
                                   (int64_t)tempmm, (int64_t)descA->mb, (int64_t)descA->mb,
                                   &alpha, A_h, CUDA_R_16F, (int64_t)ldam,
                                           B_h, CUDA_R_16F, (int64_t)ldan,
                                   &beta,  C,   CUDA_R_16F, (int64_t)ldam,
                                   CUDA_R_16F, CUBLAS_GEMM_DEFAULT_TENSOR_OP);
                 break;

            default:
                 fprintf(stderr, "tensor_gemm should 1, 2 or 3\n");
        }

        /* After last local GEMM convert C from half to single */
        if( n-1 == k && ( 2 == tensor_gemm || 3 == tensor_gemm ) ) {
            /* Get the temporary buffer on GPU */
            parsec_potrf_workspace_t *_ws_single1 = (parsec_potrf_workspace_t *)ws_single1;
            parsec_potrf_stream_workspace_t stream_single1 = lookup_gpu_workspace(gpu_device, gpu_stream, _ws_single1);
            C_s = (float *)stream_single1.gpu_buffer;
            assert(NULL != C_s);

            /* Convert datatype */
            half2float_GPU( descA->mb, descA->nb, C, descA->mb, C_s, descA->mb, parsec_body.stream );

            /* Copy C_s to C */
            memcpy_float_GPU( descA->mb, descA->nb, C_s, C, parsec_body.stream );
        }
    
}

#endif

}
END

BODY
{
    int tempmm = m == descA->mt-1 ? descA->m - m * descA->mb : descA->mb;
    int ldam = descA->mb; 
    int ldan = descA->mb; 

    CORE_sgemm(PlasmaNoTrans, PlasmaTrans,
               tempmm, descA->mb, descA->mb,
               (float)-1.0, A /*A(m, k)*/, ldam,
                            B /*A(n, k)*/, ldan,
               (float) 1.0, C /*A(m, n)*/, ldam);

    printlog("CORE_sgemm( %d, %d, %d )\n\t( %s, %s, %d, %d, %d, %f, A(%d,%d)[%p], %d, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             m, n, k,
             plasma_const( PlasmaNoTrans ),  plasma_const( PlasmaTrans ),
             tempmm, descA->mb, descA->mb,
             -1.0, m, k, A, ldam,
                   n, k, B, ldan,
              1.0, m, n, C, ldam);
}
END
