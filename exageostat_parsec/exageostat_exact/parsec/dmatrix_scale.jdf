extern "C" %{
/*
 * Copyright (c) 2017-2018 The Universiy of Tennessee and The Universiy
 *                         of Tennessee Research Foundation. All rights
 *                         reserved.
 */

#include "mix_precision_internal.h"

#if defined(EXAGEOSTAT_USE_CUDA)
#include "cudacore/include/exageostatcudacore.h"
#endif

/* Print more info */
static int print_more_gpu = 0;
static int print_more = 0;

%}

descA                [ type = "parsec_tiled_matrix_dc_t *" ]

nb_cuda_devices      [ type = "int"   hidden = on default = 0 ]
cuda_device_index    [ type = "int *" hidden = on default = "NULL"]


/**************************************************
 *               bind_A                     *
 **************************************************/
bind_A(m, n)

// Execution space
n = 0 .. descA->lnt-1
m = %{ return ((descA->m == descA->n)? n: 0); %} .. descA->lmt-1

// Parallel partitioning
:descA(m, n)

READ A <- descA(m, n)
       -> A Task(m, n)

BODY
{
#if defined(EXAGEOSTAT_USE_CUDA)
	if( nb_cuda_devices > 0 ) {
		int g = my_gpu_load( m, n, descA->nt, nb_cuda_devices, nb_cuda_devices, nb_cuda_devices);
		parsec_advise_data_on_device( _f_A->original,
				cuda_device_index[g],
				PARSEC_DEV_DATA_ADVICE_PREFERRED_DEVICE );
	}
#endif
}
END


Task(m, n)

n = 0 .. descA->lnt-1
m = %{ return ((descA->m == descA->n)? n: 0); %} .. descA->lmt-1 

: descA(m, n)

RW A <- A bind_A(m, n)
     -> descA(m, n)

BODY [type=CUDA weight=m+n+1]
{
#if defined(EXAGEOSTAT_USE_CUDA) 
	if( print_more_gpu ) printf("%d %d : GPU matrix scale\n", m, n);
        int m0 = m * descA->mb;
        int n0 = n * descA->nb;

        cublasSetKernelStream( parsec_body.stream );

        if( m != n ) {
		dndscale_array(A, A, A, descA->mb, descA->nb, m0, n0, parsec_body.stream);
	} else {
		ddscale_array(A, descA->mb, descA->nb, m0, n0, parsec_body.stream);
	}
#endif
}
END

BODY
{
	if( print_more ) printf("%d %d : CPU matrix scale\n", m, n);
	int m0 = m * descA->mb;
	int n0 = n * descA->nb;

        if( m != n ) {
		core_dndscale (A, A, A, descA->mb, descA->nb, m0, n0);
        } else {
		core_ddscale (A, descA->mb, descA->nb, m0, n0);
        }
}
END

extern "C" %{

/**
 * @param [in] dcA:    the data, already distributed and allocated
 * @return the parsec object to schedule.
 */
parsec_taskpool_t*
parsec_dmatrix_scale_New(parsec_context_t *parsec,
		parsec_tiled_matrix_dc_t *dcA)
{
    parsec_taskpool_t* dmatrix_scale_taskpool;
    parsec_dmatrix_scale_taskpool_t* taskpool = NULL;

#if defined(EXAGEOSTAT_USE_CUDA)
    /** Find all CUDA devices */
    int nb = 0;
    for(int i = 0; i < (int)parsec_nb_devices; i++) {
        parsec_device_module_t *device = parsec_mca_device_get(i);
        if( PARSEC_DEV_CUDA == device->type ) {
            nb++;
        }
    }

    if(nb == 0) {
        char hostname[256];
        gethostname(hostname, 256);
        if( print_more_gpu && 0 == dcA->super.myrank ) {
            fprintf(stderr, "\nWarnning: No CUDA device found on rank %d on %s\n\n",
                    parsec->my_rank, hostname);
        }
    }

    int *dev_index = (int*)malloc(nb * sizeof(int));
    nb = 0;
    for(int i = 0; i < (int)parsec_nb_devices; i++) {
        parsec_device_module_t *device = parsec_mca_device_get(i);
        if( PARSEC_DEV_CUDA == device->type ) {
            dev_index[nb++] = device->device_index;
        }
    }
#endif

    taskpool = parsec_dmatrix_scale_new( dcA );
    dmatrix_scale_taskpool = (parsec_taskpool_t*)taskpool;

#if defined(EXAGEOSTAT_USE_CUDA)
    taskpool->_g_nb_cuda_devices = nb;
    taskpool->_g_cuda_device_index = dev_index;
#endif

    parsec_matrix_add2arena(taskpool->arenas[PARSEC_dmatrix_scale_DEFAULT_ARENA],
                            parsec_datatype_double_t, matrix_UpperLower,
                            1, dcA->mb, dcA->nb, dcA->mb,
                            PARSEC_ARENA_ALIGNMENT_SSE, -1 );

    return dmatrix_scale_taskpool;
}

/**
 * @param [inout] the parsec object to destroy
*/
void parsec_dmatrix_scale_Destruct(parsec_taskpool_t *taskpool)
{
    parsec_dmatrix_scale_taskpool_t *tp = (parsec_dmatrix_scale_taskpool_t *)taskpool;

#if defined(EXAGEOSTAT_USE_CUDA)
    if( tp->_g_nb_cuda_devices > 0 ) {
        if( NULL != tp->_g_cuda_device_index )
            free(tp->_g_cuda_device_index);
    }
#endif

    parsec_matrix_del2arena(tp->arenas[PARSEC_dmatrix_scale_DEFAULT_ARENA]);
    parsec_taskpool_free(taskpool);
}

/**
 * @param [inout] dcA: the data, already distributed and allocated
 */
int parsec_dmatrix_scale(parsec_context_t *parsec,
                         parsec_tiled_matrix_dc_t *dcA)
{
    parsec_taskpool_t *parsec_dmatrix_scale = NULL;

    parsec_dmatrix_scale = parsec_dmatrix_scale_New( parsec, dcA ); 

    if( parsec_dmatrix_scale != NULL ){
        parsec_context_add_taskpool(parsec, parsec_dmatrix_scale);
        parsec_context_start(parsec);
        parsec_context_wait(parsec);
        parsec_dmatrix_scale_Destruct(parsec_dmatrix_scale);
    }

    return 0;
}

%}
